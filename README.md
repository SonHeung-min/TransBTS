# TransBTS & TransBTSV2: Multimodal Brain Tumor Segmentation

![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)
![Python](https://img.shields.io/badge/python-3.7-blue)
![PyTorch](https://img.shields.io/badge/pytorch-1.6.0-green)

> **LÆ°u Ã½:** ÄÃ¢y lÃ  repository cÃ¡ nhÃ¢n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ nghiÃªn cá»©u, triá»ƒn khai vÃ  tÃ¹y chá»‰nh láº¡i mÃ£ nguá»“n gá»‘c cá»§a cÃ¡c bÃ i bÃ¡o khoa há»c.

Dá»± Ã¡n nÃ y lÃ  cÃ i Ä‘áº·t thá»±c nghiá»‡m cho:
1.  [**TransBTS**](https://arxiv.org/abs/2103.04430): Multimodal Brain Tumor Segmentation Using Transformer (MICCAI 2021).
2.  [**TransBTSV2**](https://arxiv.org/abs/2201.12785): Towards Better and More Efficient Volumetric Segmentation of Medical Images.

---

## ğŸ“‘ Má»¥c lá»¥c (Table of Contents)
- [Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n-project-structure)
- [YÃªu cáº§u há»‡ thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng-requirements)
- [Dá»¯ liá»‡u](#-dá»¯-liá»‡u-dataset)
- [HÆ°á»›ng dáº«n sá»­ dá»¥ng](#-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng-usage)
    - [1. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u](#1-tiá»n-xá»­-lÃ½-dá»¯-liá»‡u-preprocessing)
    - [2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh](#2-huáº¥n-luyá»‡n-mÃ´-hÃ¬nh-training)
    - [3. Kiá»ƒm thá»­ & ÄÃ¡nh giÃ¡](#3-kiá»ƒm-thá»­--Ä‘Ã¡nh-giÃ¡-testing)
- [TrÃ­ch dáº«n & Báº£n quyá»n](#-trÃ­ch-dáº«n--báº£n-quyá»n-citation--license)

---

## ğŸ“‚ Cáº¥u trÃºc dá»± Ã¡n (Project Structure)
```
TransBTS/
â”œâ”€â”€ data/                       # Quáº£n lÃ½ dá»¯ liá»‡u & Tiá»n xá»­ lÃ½
â”‚   â”œâ”€â”€ BraTS.py                # Dataset loader cho BraTS
â”‚   â”œâ”€â”€ preprocess.py           # Script tiá»n xá»­ lÃ½ (convert .nii -> .pkl)
â”‚   â”œâ”€â”€ train.txt               # Danh sÃ¡ch file huáº¥n luyá»‡n
â”‚   â””â”€â”€ valid.txt               # Danh sÃ¡ch file validation
â”œâ”€â”€ models/                     # Kiáº¿n trÃºc mÃ´ hÃ¬nh
â”‚   â”œâ”€â”€ TransBTS/               # MÃ£ nguá»“n TransBTS (MICCAI 2021)
â”‚   â”‚   â”œâ”€â”€ IntmdSequential.py                  # CÃ¡c lá»›p trung gian
â”‚   â”‚   â”œâ”€â”€ PositionalEncoding.py               # MÃ£ hÃ³a vá»‹ trÃ­
â”‚   â”‚   â”œâ”€â”€ TransBTS_downsample8x_skipconnection.py # Kiáº¿n trÃºc chÃ­nh
â”‚   â”‚   â”œâ”€â”€ Transformer.py                      # Module Transformer
â”‚   â”‚   â”œâ”€â”€ Unet_skipconnection.py              # Pháº§n U-Net
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ TransBTSV2/             # MÃ£ nguá»“n TransBTSV2
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ criterions.py           # CÃ¡c hÃ m Loss function
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ utils/                      # CÃ¡c hÃ m tiá»‡n Ã­ch há»— trá»£
â”œâ”€â”€ figures/                    # Biá»ƒu Ä‘á»“ vÃ  hÃ¬nh áº£nh minh há»a
â”œâ”€â”€ train.py                    # Script huáº¥n luyá»‡n chÃ­nh
â”œâ”€â”€ test.py                     # Script kiá»ƒm thá»­/Ä‘Ã¡nh giÃ¡
â”œâ”€â”€ predict.py                  # Script dá»± Ä‘oÃ¡n (inference)
â”œâ”€â”€ LICENSE                     # ThÃ´ng tin báº£n quyá»n
â””â”€â”€ README.md                   # TÃ i liá»‡u hÆ°á»›ng dáº«n (File nÃ y)
```

---

## ğŸ›  YÃªu cáº§u há»‡ thá»‘ng (Requirements)
Äá»ƒ cháº¡y mÃ£ nguá»“n nÃ y, vui lÃ²ng Ä‘áº£m báº£o mÃ´i trÆ°á»ng Ä‘Ã£ cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n sau:
*   Python 3.7
*   PyTorch 1.6.0
*   TorchiVision 0.7.0
*   Pickle
*   Nibabel

CÃ i Ä‘áº·t nhanh cÃ¡c thÆ° viá»‡n phá»¥ thuá»™c:
```bash
pip install torch==1.6.0 torchvision==0.7.0 nibabel pickle-mixin
```

---

## ğŸ’¾ Dá»¯ liá»‡u (Dataset)
CÃ¡c bá»™ dá»¯ liá»‡u y táº¿ Ä‘Æ°á»£c há»— trá»£ vÃ  sá»­ dá»¥ng trong nghiÃªn cá»©u nÃ y:

| Dataset | MÃ´ táº£ | Link Táº£i |
| :--- | :--- | :--- |
| **BraTS 2019/2020** | Khá»‘i u nÃ£o Ä‘a phÆ°Æ¡ng thá»©c | [Download](https://ipp.cbica.upenn.edu/) |
| **LiTS 2017** | Khá»‘i u gan | [Download](https://competitions.codalab.org/competitions/17094) |
| **KiTS 2019** | Khá»‘i u tháº­n | [Download](https://kits19.grand-challenge.org/data/) |

---

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng (Usage)

### 1. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (Preprocessing)
Äá»‘i vá»›i dá»¯ liá»‡u **BraTS** (2019/2020), sau khi táº£i vá», hÃ£y cháº¡y script sau Ä‘á»ƒ chuyá»ƒn Ä‘á»•i file `.nii` sang Ä‘á»‹nh dáº¡ng `.pkl` tá»‘i Æ°u hÃ³a cho viá»‡c load dá»¯ liá»‡u vÃ  chuáº©n hÃ³a intensity.
**LÆ°u Ã½:** Script náº±m trong thÆ° má»¥c `data/`. Báº¡n cáº§n thay Ä‘á»•i Ä‘Æ°á»ng dáº«n (path) trong file `data/preprocess.py` trá» tá»›i thÆ° má»¥c chá»©a dá»¯ liá»‡u Ä‘Ã£ táº£i vá» cá»§a mÃ¬nh trÆ°á»›c khi cháº¡y.

```bash
python3 data/preprocess.py
```

### 2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh (Training)
Lá»‡nh dÆ°á»›i Ä‘Ã¢y sáº½ khá»Ÿi cháº¡y quÃ¡ trÃ¬nh huáº¥n luyá»‡n phÃ¢n tÃ¡n (Distributed Training) cho TransBTS trÃªn dá»¯ liá»‡u BraTS:

```bash
python3 -m torch.distributed.launch --nproc_per_node=4 --master_port 20003 train.py
```
*   `--nproc_per_node`: Sá»‘ lÆ°á»£ng GPU sá»­ dá»¥ng (vÃ­ dá»¥: 4).
*   `--master_port`: Cá»•ng giao tiáº¿p cho process group.

### 3. Kiá»ƒm thá»­ & ÄÃ¡nh giÃ¡ (Testing)
Äá»ƒ thá»±c hiá»‡n kiá»ƒm thá»­ vá»›i mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n:

```bash
python3 test.py
```
Sau khi tiáº¿n trÃ¬nh káº¿t thÃºc, file submission cÃ³ thá»ƒ Ä‘Æ°á»£c ná»™p lÃªn trang chá»§ [BraTS Challenge](https://ipp.cbica.upenn.edu/) Ä‘á»ƒ láº¥y káº¿t quáº£ Dice score chÃ­nh thá»©c.

---

## ğŸ“œ TrÃ­ch dáº«n & Báº£n quyá»n (Citation & License)
Dá»± Ã¡n nÃ y tuÃ¢n theo giáº¥y phÃ©p [Apache 2.0](./LICENSE).
Náº¿u báº¡n sá»­ dá»¥ng mÃ£ nguá»“n hoáº·c Ã½ tÆ°á»Ÿng tá»« TransBTS/TransBTSV2, vui lÃ²ng trÃ­ch dáº«n cÃ¡c bÃ i bÃ¡o gá»‘c dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tÃ´n trá»ng tÃ¡c giáº£:

**TransBTS (MICCAI 2021)**:
```bibtex
@inproceedings{wang2021transbts,
  title={TransBTS: Multimodal Brain Tumor Segmentation Using Transformer},
  author={Wang, Wenxuan and Chen, Chen and Ding, Meng and Yu, Hong and Zha, Sen and Li, Jiangyun},
  booktitle={MICCAI 2021: 24th International Conference},
  pages={109--119},
  year={2021},
  organization={Springer}
}
```

**TransBTSV2 (arXiv)**:
```bibtex
@article{li2022transbtsv2,
  title={TransBTSV2: Wider Instead of Deeper Transformer for Medical Image Segmentation},
  author={Li, Jiangyun and Wang, Wenxuan and Chen, Chen and Zhang, Tianxiang and Zha, Sen and Yu, Hong and Wang, Jing},
  journal={arXiv preprint arXiv:2201.12785},
  year={2022}
}
```

---
*Reference implementations*:
*   [setr-pytorch](https://github.com/gupta-abhay/setr-pytorch)
*   [BraTS2017](https://github.com/MIC-DKFZ/BraTS2017)
